[
  {
    "instruction": "Explain the v2 MoE architecture.",
    "context": "Deepseek-V2 utilizes Multi-head Latent Attention (MLA).",
    "chosen": "Deepseek-V2 optimizes training via MLA, compressing KV cache significantly.",
    "rejected": "Deepseek-V2 is a standard dense transformer model.",
    "v2_metadata": {
      "diversity": "technical",
      "phase": "1b"
    }
  },
  {
    "instruction": "How does Phase 1b improve data?",
    "context": "Phase 1b focuses on Subjective Expansion.",
    "chosen": "Phase 1b uses a Knowledge Graph to inject global context.",
    "rejected": "Phase 1b is primarily for pre-training on raw text.",
    "v2_metadata": {
      "diversity": "simple",
      "phase": "1b"
    }
  }
]